---
title: 行为识别
date: 2018-06-17 20:11:28
permalink: action
categories: 小论文
tags: [行为识别,LSTM]
description: 行为识别两大流派
image: https://ws1.sinaimg.cn/large/006tNc79ly1fsowvplb0cj31kw11xhdz.jpg
cover: https://ws1.sinaimg.cn/large/006tNc79ly1fsowvplb0cj31kw11xhdz.jpg
mp3: http://link.hhtjim.com/163/436147067.mp3
---
<p class="description"></p>

<!-- more -->

行为识别两大流派：(1)Two-Stream     (2)C3D
![Two-Stream](https://upload-images.jianshu.io/upload_images/5806342-8a4133bac606e921.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## Two-Stream流派思想：

利用**视频帧图像(spatial)和根据帧图像提取到的光流场图像(temporal flow)**，各自训练出一个模型，在网络产生结果后，对网络做一个**融合(Fusion)**。这两个模型分别表示**静态信息和短时序信息**。它能有效解决从一张图就可以识别出行为的类别。

> 光流是什么？
> 光流能通过速度提供物体的**远近信息**，远处似静止，近处似脱兔。此外，还能提供**角度信息**，与眼睛呈90°的物体运动最快，角度小到0°的时候感受不到它的运动。

Two-Stream代表算法：
(1) Two-Stream Convolutional Networks for Action Recognition in Videos(2014NIPS)
- Two-Stream方法首次在这篇文章中被提出来，他的原理是对视频序列中**每两帧计算密集光流**，得到密集光流序列(temporal信息)，然后对RGB图像(spatial)和密集光流(temporal)分别训练CNN模型。两个分支的网络分别对动作进行判断，最后对两个网络的class score进行融合(fusion)，论文中fusion方法是直接平均和SVM方法，最终得到分类结果。两个分支使用相同的2DCNN网咯，结构图如下：![Two-Stream architecture for video classfication](http://upload-images.jianshu.io/upload_images/5806342-263860d1555076ab.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
- 实验结果：UCF-101 88，HMDB51 59.4%

(2) Convolutional Two-Stream Network Fusion for Video Action Recognition(2016CVPR)
- 论文主要内容在上一篇文章Two-Stream network的基础上，利用CNN网络对RGB图像和密集光流进行融合，效果提高了准确率。此外，该文章还将双流网络都换成VGG-16 network。网络结构如下：![](http://upload-images.jianshu.io/upload_images/5806342-4476d006983f3283.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
  (3)Temporal Segment Networks: Towards Good Practices for Deep Action Recognition
  目前精度最高的算法：TSN
  做的工作是提出新的Two-Stream网络，具体做的工作如下：
1. 改变输入数据的类型，不再是传统上的RGB+Flow这种输入，而是采用四种输入数据，即RGB image+optical flow+RGB difference+warped optical flow来研究对动作检测效果的影响。
    ![四种输入数据类型](http://upload-images.jianshu.io/upload_images/5806342-2cc052feef34b575?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
2. 改进网络结构，提出TSN网络，实质上也是Two-Stream模式，网络结构如下图所示：
    ![TSN网络结构](http://upload-images.jianshu.io/upload_images/5806342-85e12c8c3ee823e5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)网络部分是由双路CNN组成，分别为spatial convnet和temporal convnet。这两个网络使用的都是BN-Inception（GoogleNet网络的演变体）。
>**注意点：**
>1. spatial convnet是以RGB Image作为输入，temporal convnet是以a stack of consecutive optical flow fields（一堆连续的的光流场）作为输入。
>2. 整个temporal segment network(TSN网络，包含多个spatial convnet和temporal convnet)的数据是不是单帧图像，而是采用a sequence of short snippets sparsely sampled from the entire video(从短视频序列中稀疏采样)得到的，目的是为了获得long-range temporal structure(长距离时间结构)所做的改进。

总体流程过一遍就是：

输入一段视频**V**，被分为**K段(segment)**，从每个segment中稀疏采样**一个片段(snippet)**。不同片段(snippet)的类别得分用段共识函数（The segmental consensus function）进行融合产生段共识（segmental consensus），这是一个视频级的预测。然后对所有模式的预测融合产生最终的预测结果。

具体细节：

> 注意点：接下来“段”代表文章中的“segment”，“片段”代表文章中的“snippet”

给定一段视频**V**，把它按相等间隔分为**K**段{$S_1,S_2,...,S_K$}。接着，TSN按照如下方式对一些列片段进行建模：
$$
TSN(T_1,T_2,...T_K)=H(G(F(T_1;W),F(T_2;W),...,F(T_K;W)))
$$

- $(T_1,T_2,...,T_K)$代表**片段序列**，每个片段$T_K$从它所对应的$S_K$中中随机采样得到。
- $F(T_K;W)$函数表示采用$W$作为参数的卷积网络作用于短片段$T_K$，函数返回$T_K$相对于所有类别的得分。
- 段共识函数$G$(The segmental consensus function)结合多个短片段的类别得分输出以获得他们之间关于类别假设的共识 。
- 基于这个共识，预测函数$H$预测整段视频属于每个行为类别的概率 。

测试UCF-101数据集

仅rgb：

```
python tools/eval_net.py ucf101 1 rgb /data3/UCF-all-in-one/ucf_frame/ \ 
models/ucf101/tsn_bn_inception_rgb_deploy.prototxt \
models/ucf101_split_1_tsn_rgb_reference_bn_inception.caffemodel \
--num_worker 1 --save_scores rgb_score
```

![rgb](https://upload-images.jianshu.io/upload_images/5806342-e1686e271ab9b2dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

仅flow：

```
python tools/eval_net.py ucf101 1 flow /data3/UCF-all-in-one/ucf_transed/ \
models/ucf101/tsn_bn_inception_flow_deploy.prototxt \
models/ucf101_split_1_tsn_flow_reference_bn_inception.caffemodel \
--num_worker 1 --save_scores ucf101/flow_score
```

![flow](https://upload-images.jianshu.io/upload_images/5806342-0477cbb7410453c8.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

fusion：

```
python tools/eval_scores.py ucf101/rgb_score.npz ucf101/flow_score.npz --score_weights 1 1.5
```

![fusion](https://upload-images.jianshu.io/upload_images/5806342-c203485e86d7312d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

##C3D算法：

![C3D](https://upload-images.jianshu.io/upload_images/5806342-681d8053f86d47d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

C3D卷积网络是facebook公司提出的端到端的行为识别网络。可以直接处理视频。下面介绍一下2D与3D的区别：![speed](http://ofltv9hb9.bkt.clouddn.com/image/c3d/2d3dconv.png)

a)和b)分别为2D卷积用于单通道图像和多通道图像的情况（此处多通道图像可以指同一张图片的3个颜色通道，也指多张堆叠在一起的图片，即一小段视频），对于一个滤波器，输出为一张二维的特征图，多通道的信息被完全压缩了。而c)中的3D卷积的输出仍然为3D的特征图。 3D卷积核是在空间和时间维度上进行操作，所以能捕捉到视频流的运动信息。

C3D网络结构：

![speed](http://ofltv9hb9.bkt.clouddn.com/image/c3d/c3d.png)

基于3D卷积操作，作者设计了如上图所示的C3D network结构。共有8次卷积操作，5次池化操作。其中卷积核的大小均为3∗3∗3，步长为1∗1∗1。池化核的大小为2∗2∗2,步长为2∗2∗2，但第一层池化除外，其大小和步长均为1∗2∗2。这是为了不过早缩减时序上的长度。最终网络在经过两次全连接层和softmax层后就得到了最终的输出结果。网络的输入尺寸为3∗16∗112∗112，即一次输入16帧图像。 

实验结果：

行为识别用的数据库是UCF101,C3D+SVM的结果达到了85.2%。 

![out](D:\scorpio\project\C3D-keras-master\Videos\out.gif)

| 算法                | 介绍                                                         | 优缺点                                     |
| :------------------ | ------------------------------------------------------------ | ------------------------------------------ |
| iDT(传统方法)       | 深度学习领域外效果最好的方法。由DT算法而来。DT算法基本思路利用光流场来获取视频序列中的一些轨迹，再沿着轨迹提取HOF，HOG，MBH， trajectory4 四种特征。HOF基于灰度图计算，另外几个基于optical(密集光流)计算。最后对其进行特征编码，再基于编码结果训练SVM分类器。iDT利用前后两帧之间的光流和surf关键点进行匹配，从而消除/减弱相机运动带来的影响。 | 优点：稳定性最高，可靠性高 缺点：速度慢    |
| Two-Stream(双流CNN) | 基本原理为对视频序列中每两帧计算密集光流，得到密集光流的序列（即temporal信息）。然后对于视频图像（spatial）和密集光流（temporal）分别训练CNN模型，两个分支的网络分别对动作的类别进行判断，最后直接对两个网络的class score进行fusion（包括直接平均和svm两种方法），得到最终的分类结果。 | 优点：精度高UCF-101-96%缺点：速度慢，20fps |
| C3D(3D卷积)         | 通过3D卷积操作核去提取视频数据的时间和空间特征。这些3D特征提取器在空间和时间维度上操作，因此可以捕捉视频流的运动信息 。然后基于3D卷积特征提取器构造了一个3D卷积神经网络。这个架构可以从连续视频帧中产生多通道的信息，然后在每一个通道都分离地进行卷积和下采样操作。最后将所有通道的信息组合起来得到最终的特征描述。 | 优点：速度快300fps缺点：精度低UCF-101-85%  |









